{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Imports:\n",
    "\n",
    "import torch  \n",
    "from datasets import load_dataset, Dataset \n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments  \n",
    "from peft import LoraConfig, get_peft_model, AutoPeftModelForSequenceClassification  \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context'],\n",
      "        num_rows: 10269\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context'],\n",
      "        num_rows: 1283\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context'],\n",
      "        num_rows: 1284\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "datasets = load_dataset(\"liar\",trust_remote_code=True)\n",
    "\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>statement</th>\n",
       "      <th>subject</th>\n",
       "      <th>speaker</th>\n",
       "      <th>job_title</th>\n",
       "      <th>state_info</th>\n",
       "      <th>party_affiliation</th>\n",
       "      <th>barely_true_counts</th>\n",
       "      <th>false_counts</th>\n",
       "      <th>half_true_counts</th>\n",
       "      <th>mostly_true_counts</th>\n",
       "      <th>pants_on_fire_counts</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3358.json</td>\n",
       "      <td>1</td>\n",
       "      <td>Thirty-five states have accepted high-speed in...</td>\n",
       "      <td>stimulus,transportation</td>\n",
       "      <td>raymond-lahood</td>\n",
       "      <td>Secretary, U.S. Department of Transportation</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a speech to the American Association of State ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8048.json</td>\n",
       "      <td>2</td>\n",
       "      <td>Since I was elected, crime rates have been at ...</td>\n",
       "      <td>crime</td>\n",
       "      <td>bill-foster</td>\n",
       "      <td>Mayor, St. Petersburg</td>\n",
       "      <td>Florida</td>\n",
       "      <td>republican</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a campaign brochure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8280.json</td>\n",
       "      <td>5</td>\n",
       "      <td>Warren Buffett recently said Scrap Obamacare a...</td>\n",
       "      <td>health-care,pundits</td>\n",
       "      <td>blog-posting</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>none</td>\n",
       "      <td>7.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>posts on the Internet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>481.json</td>\n",
       "      <td>2</td>\n",
       "      <td>Eliminating earmarks \"would make barely a drop...</td>\n",
       "      <td>federal-budget</td>\n",
       "      <td>bob-barr</td>\n",
       "      <td>Runs a consulting firm, Liberty Strategies LLC</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>libertarian</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a news conference announcing his Libertarian c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12651.json</td>\n",
       "      <td>4</td>\n",
       "      <td>Rather than work to secure the border, (Marco ...</td>\n",
       "      <td>immigration</td>\n",
       "      <td>carlos-beruff</td>\n",
       "      <td>Developer</td>\n",
       "      <td>Florida</td>\n",
       "      <td>republican</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a statement to press</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  label                                          statement  \\\n",
       "0   3358.json      1  Thirty-five states have accepted high-speed in...   \n",
       "1   8048.json      2  Since I was elected, crime rates have been at ...   \n",
       "2   8280.json      5  Warren Buffett recently said Scrap Obamacare a...   \n",
       "3    481.json      2  Eliminating earmarks \"would make barely a drop...   \n",
       "4  12651.json      4  Rather than work to secure the border, (Marco ...   \n",
       "\n",
       "                   subject         speaker  \\\n",
       "0  stimulus,transportation  raymond-lahood   \n",
       "1                    crime     bill-foster   \n",
       "2      health-care,pundits    blog-posting   \n",
       "3           federal-budget        bob-barr   \n",
       "4              immigration   carlos-beruff   \n",
       "\n",
       "                                        job_title state_info  \\\n",
       "0    Secretary, U.S. Department of Transportation   Illinois   \n",
       "1                           Mayor, St. Petersburg    Florida   \n",
       "2                                                              \n",
       "3  Runs a consulting firm, Liberty Strategies LLC    Georgia   \n",
       "4                                       Developer    Florida   \n",
       "\n",
       "  party_affiliation  barely_true_counts  false_counts  half_true_counts  \\\n",
       "0        republican                 0.0           1.0               1.0   \n",
       "1        republican                 1.0           0.0               2.0   \n",
       "2              none                 7.0          19.0               3.0   \n",
       "3       libertarian                 0.0           0.0               0.0   \n",
       "4        republican                 3.0           1.0               0.0   \n",
       "\n",
       "   mostly_true_counts  pants_on_fire_counts  \\\n",
       "0                 1.0                   0.0   \n",
       "1                 2.0                   0.0   \n",
       "2                 5.0                  44.0   \n",
       "3                 1.0                   0.0   \n",
       "4                 0.0                   0.0   \n",
       "\n",
       "                                             context  \n",
       "0  a speech to the American Association of State ...  \n",
       "1                                a campaign brochure  \n",
       "2                              posts on the Internet  \n",
       "3  a news conference announcing his Libertarian c...  \n",
       "4                               a statement to press  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff=pd.DataFrame(datasets[\"train\"].shuffle(seed=42).select(range(5)))\n",
    "dff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = ['id', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', \n",
    "                     'barely_true_counts', 'false_counts', 'half_true_counts', \n",
    "                     'mostly_true_counts', 'pants_on_fire_counts', 'context']\n",
    "\n",
    "dataset = datasets.remove_columns(columns_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_ck=\"distilbert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ck)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_ck,num_labels =6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fun(example):\n",
    "    return tokenizer(example[\"statement\"],padding=\"max_length\",truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_fun,batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Liardataset(Dataset):\n",
    "    def __init__(self,tokenized_data):\n",
    "        self.data = tokenized_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[\"input_ids\"])\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return {\n",
    "            \"input_ids\" : torch.tensor(self.data[\"input_ids\"][idx],dtype = torch.long),\n",
    "            \"attention_mask\": torch.tensor(self.data[\"attention_mask\"][idx], dtype=torch.long),\n",
    "            \"labels\": torch.tensor(self.data[\"label\"][idx], dtype=torch.long),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2758,  1996,  ...,     0,     0,     0],\n",
      "        [  101,  2758,  1037,  ...,     0,     0,     0],\n",
      "        [  101,  2343,  8112,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  2062,  2084,  ...,     0,     0,     0],\n",
      "        [  101,  5392,  2816,  ...,     0,     0,     0],\n",
      "        [  101,  2758, 12163,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 2, 2, 2, 4, 1, 0, 4, 1, 0, 4, 1, 1, 4, 3, 1])}\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_dataset = Liardataset(tokenized_dataset[\"train\"])\n",
    "val_dataset = Liardataset(tokenized_dataset[\"validation\"])\n",
    "test_dataset = Liardataset(tokenized_dataset[\"test\"])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset,batch_size=batch_size,shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False)\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "print(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained Model Accuracy: 0.1885\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        batch = {key:vals.to(device) for key,vals in batch.items()}\n",
    "        outputs = model(input_ids = batch[\"input_ids\"], attention_mask = batch[\"attention_mask\"])\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits,dim=-1)\n",
    "\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "accuracy = metric.compute()\n",
    "print(f\"Pretrained Model Accuracy: {accuracy['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 816,390 || all params: 67,774,476 || trainable%: 1.2046\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules= [\"q_lin\",\"v_lin\",\"k_lin\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "if hasattr(model,\"lora_config\"):\n",
    "    model=model.unload()\n",
    "peft_model = get_peft_model(model,lora_config)\n",
    "\n",
    "peft_model.to(device)\n",
    "\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert.embeddings.word_embeddings.weight\n",
      "distilbert.embeddings.position_embeddings.weight\n",
      "distilbert.embeddings.LayerNorm.weight\n",
      "distilbert.embeddings.LayerNorm.bias\n",
      "distilbert.transformer.layer.0.attention.q_lin.base_layer.weight\n",
      "distilbert.transformer.layer.0.attention.q_lin.base_layer.bias\n",
      "distilbert.transformer.layer.0.attention.q_lin.lora_A.default.weight\n",
      "distilbert.transformer.layer.0.attention.q_lin.lora_B.default.weight\n",
      "distilbert.transformer.layer.0.attention.k_lin.base_layer.weight\n",
      "distilbert.transformer.layer.0.attention.k_lin.base_layer.bias\n",
      "distilbert.transformer.layer.0.attention.k_lin.lora_A.default.weight\n",
      "distilbert.transformer.layer.0.attention.k_lin.lora_B.default.weight\n",
      "distilbert.transformer.layer.0.attention.v_lin.base_layer.weight\n",
      "distilbert.transformer.layer.0.attention.v_lin.base_layer.bias\n",
      "distilbert.transformer.layer.0.attention.v_lin.lora_A.default.weight\n",
      "distilbert.transformer.layer.0.attention.v_lin.lora_B.default.weight\n",
      "distilbert.transformer.layer.0.attention.out_lin.weight\n",
      "distilbert.transformer.layer.0.attention.out_lin.bias\n",
      "distilbert.transformer.layer.0.sa_layer_norm.weight\n",
      "distilbert.transformer.layer.0.sa_layer_norm.bias\n",
      "distilbert.transformer.layer.0.ffn.lin1.weight\n",
      "distilbert.transformer.layer.0.ffn.lin1.bias\n",
      "distilbert.transformer.layer.0.ffn.lin2.weight\n",
      "distilbert.transformer.layer.0.ffn.lin2.bias\n",
      "distilbert.transformer.layer.0.output_layer_norm.weight\n",
      "distilbert.transformer.layer.0.output_layer_norm.bias\n",
      "distilbert.transformer.layer.1.attention.q_lin.base_layer.weight\n",
      "distilbert.transformer.layer.1.attention.q_lin.base_layer.bias\n",
      "distilbert.transformer.layer.1.attention.q_lin.lora_A.default.weight\n",
      "distilbert.transformer.layer.1.attention.q_lin.lora_B.default.weight\n",
      "distilbert.transformer.layer.1.attention.k_lin.base_layer.weight\n",
      "distilbert.transformer.layer.1.attention.k_lin.base_layer.bias\n",
      "distilbert.transformer.layer.1.attention.k_lin.lora_A.default.weight\n",
      "distilbert.transformer.layer.1.attention.k_lin.lora_B.default.weight\n",
      "distilbert.transformer.layer.1.attention.v_lin.base_layer.weight\n",
      "distilbert.transformer.layer.1.attention.v_lin.base_layer.bias\n",
      "distilbert.transformer.layer.1.attention.v_lin.lora_A.default.weight\n",
      "distilbert.transformer.layer.1.attention.v_lin.lora_B.default.weight\n",
      "distilbert.transformer.layer.1.attention.out_lin.weight\n",
      "distilbert.transformer.layer.1.attention.out_lin.bias\n",
      "distilbert.transformer.layer.1.sa_layer_norm.weight\n",
      "distilbert.transformer.layer.1.sa_layer_norm.bias\n",
      "distilbert.transformer.layer.1.ffn.lin1.weight\n",
      "distilbert.transformer.layer.1.ffn.lin1.bias\n",
      "distilbert.transformer.layer.1.ffn.lin2.weight\n",
      "distilbert.transformer.layer.1.ffn.lin2.bias\n",
      "distilbert.transformer.layer.1.output_layer_norm.weight\n",
      "distilbert.transformer.layer.1.output_layer_norm.bias\n",
      "distilbert.transformer.layer.2.attention.q_lin.base_layer.weight\n",
      "distilbert.transformer.layer.2.attention.q_lin.base_layer.bias\n",
      "distilbert.transformer.layer.2.attention.q_lin.lora_A.default.weight\n",
      "distilbert.transformer.layer.2.attention.q_lin.lora_B.default.weight\n",
      "distilbert.transformer.layer.2.attention.k_lin.base_layer.weight\n",
      "distilbert.transformer.layer.2.attention.k_lin.base_layer.bias\n",
      "distilbert.transformer.layer.2.attention.k_lin.lora_A.default.weight\n",
      "distilbert.transformer.layer.2.attention.k_lin.lora_B.default.weight\n",
      "distilbert.transformer.layer.2.attention.v_lin.base_layer.weight\n",
      "distilbert.transformer.layer.2.attention.v_lin.base_layer.bias\n",
      "distilbert.transformer.layer.2.attention.v_lin.lora_A.default.weight\n",
      "distilbert.transformer.layer.2.attention.v_lin.lora_B.default.weight\n",
      "distilbert.transformer.layer.2.attention.out_lin.weight\n",
      "distilbert.transformer.layer.2.attention.out_lin.bias\n",
      "distilbert.transformer.layer.2.sa_layer_norm.weight\n",
      "distilbert.transformer.layer.2.sa_layer_norm.bias\n",
      "distilbert.transformer.layer.2.ffn.lin1.weight\n",
      "distilbert.transformer.layer.2.ffn.lin1.bias\n",
      "distilbert.transformer.layer.2.ffn.lin2.weight\n",
      "distilbert.transformer.layer.2.ffn.lin2.bias\n",
      "distilbert.transformer.layer.2.output_layer_norm.weight\n",
      "distilbert.transformer.layer.2.output_layer_norm.bias\n",
      "distilbert.transformer.layer.3.attention.q_lin.base_layer.weight\n",
      "distilbert.transformer.layer.3.attention.q_lin.base_layer.bias\n",
      "distilbert.transformer.layer.3.attention.q_lin.lora_A.default.weight\n",
      "distilbert.transformer.layer.3.attention.q_lin.lora_B.default.weight\n",
      "distilbert.transformer.layer.3.attention.k_lin.base_layer.weight\n",
      "distilbert.transformer.layer.3.attention.k_lin.base_layer.bias\n",
      "distilbert.transformer.layer.3.attention.k_lin.lora_A.default.weight\n",
      "distilbert.transformer.layer.3.attention.k_lin.lora_B.default.weight\n",
      "distilbert.transformer.layer.3.attention.v_lin.base_layer.weight\n",
      "distilbert.transformer.layer.3.attention.v_lin.base_layer.bias\n",
      "distilbert.transformer.layer.3.attention.v_lin.lora_A.default.weight\n",
      "distilbert.transformer.layer.3.attention.v_lin.lora_B.default.weight\n",
      "distilbert.transformer.layer.3.attention.out_lin.weight\n",
      "distilbert.transformer.layer.3.attention.out_lin.bias\n",
      "distilbert.transformer.layer.3.sa_layer_norm.weight\n",
      "distilbert.transformer.layer.3.sa_layer_norm.bias\n",
      "distilbert.transformer.layer.3.ffn.lin1.weight\n",
      "distilbert.transformer.layer.3.ffn.lin1.bias\n",
      "distilbert.transformer.layer.3.ffn.lin2.weight\n",
      "distilbert.transformer.layer.3.ffn.lin2.bias\n",
      "distilbert.transformer.layer.3.output_layer_norm.weight\n",
      "distilbert.transformer.layer.3.output_layer_norm.bias\n",
      "distilbert.transformer.layer.4.attention.q_lin.base_layer.weight\n",
      "distilbert.transformer.layer.4.attention.q_lin.base_layer.bias\n",
      "distilbert.transformer.layer.4.attention.q_lin.lora_A.default.weight\n",
      "distilbert.transformer.layer.4.attention.q_lin.lora_B.default.weight\n",
      "distilbert.transformer.layer.4.attention.k_lin.base_layer.weight\n",
      "distilbert.transformer.layer.4.attention.k_lin.base_layer.bias\n",
      "distilbert.transformer.layer.4.attention.k_lin.lora_A.default.weight\n",
      "distilbert.transformer.layer.4.attention.k_lin.lora_B.default.weight\n",
      "distilbert.transformer.layer.4.attention.v_lin.base_layer.weight\n",
      "distilbert.transformer.layer.4.attention.v_lin.base_layer.bias\n",
      "distilbert.transformer.layer.4.attention.v_lin.lora_A.default.weight\n",
      "distilbert.transformer.layer.4.attention.v_lin.lora_B.default.weight\n",
      "distilbert.transformer.layer.4.attention.out_lin.weight\n",
      "distilbert.transformer.layer.4.attention.out_lin.bias\n",
      "distilbert.transformer.layer.4.sa_layer_norm.weight\n",
      "distilbert.transformer.layer.4.sa_layer_norm.bias\n",
      "distilbert.transformer.layer.4.ffn.lin1.weight\n",
      "distilbert.transformer.layer.4.ffn.lin1.bias\n",
      "distilbert.transformer.layer.4.ffn.lin2.weight\n",
      "distilbert.transformer.layer.4.ffn.lin2.bias\n",
      "distilbert.transformer.layer.4.output_layer_norm.weight\n",
      "distilbert.transformer.layer.4.output_layer_norm.bias\n",
      "distilbert.transformer.layer.5.attention.q_lin.base_layer.weight\n",
      "distilbert.transformer.layer.5.attention.q_lin.base_layer.bias\n",
      "distilbert.transformer.layer.5.attention.q_lin.lora_A.default.weight\n",
      "distilbert.transformer.layer.5.attention.q_lin.lora_B.default.weight\n",
      "distilbert.transformer.layer.5.attention.k_lin.base_layer.weight\n",
      "distilbert.transformer.layer.5.attention.k_lin.base_layer.bias\n",
      "distilbert.transformer.layer.5.attention.k_lin.lora_A.default.weight\n",
      "distilbert.transformer.layer.5.attention.k_lin.lora_B.default.weight\n",
      "distilbert.transformer.layer.5.attention.v_lin.base_layer.weight\n",
      "distilbert.transformer.layer.5.attention.v_lin.base_layer.bias\n",
      "distilbert.transformer.layer.5.attention.v_lin.lora_A.default.weight\n",
      "distilbert.transformer.layer.5.attention.v_lin.lora_B.default.weight\n",
      "distilbert.transformer.layer.5.attention.out_lin.weight\n",
      "distilbert.transformer.layer.5.attention.out_lin.bias\n",
      "distilbert.transformer.layer.5.sa_layer_norm.weight\n",
      "distilbert.transformer.layer.5.sa_layer_norm.bias\n",
      "distilbert.transformer.layer.5.ffn.lin1.weight\n",
      "distilbert.transformer.layer.5.ffn.lin1.bias\n",
      "distilbert.transformer.layer.5.ffn.lin2.weight\n",
      "distilbert.transformer.layer.5.ffn.lin2.bias\n",
      "distilbert.transformer.layer.5.output_layer_norm.weight\n",
      "distilbert.transformer.layer.5.output_layer_norm.bias\n",
      "pre_classifier.original_module.weight\n",
      "pre_classifier.original_module.bias\n",
      "pre_classifier.modules_to_save.default.weight\n",
      "pre_classifier.modules_to_save.default.bias\n",
      "classifier.original_module.weight\n",
      "classifier.original_module.bias\n",
      "classifier.modules_to_save.default.weight\n",
      "classifier.modules_to_save.default.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3852' max='3852' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3852/3852 13:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.695100</td>\n",
       "      <td>1.706794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.664900</td>\n",
       "      <td>1.675118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.522500</td>\n",
       "      <td>1.674011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3852, training_loss=1.6634717696923704, metrics={'train_runtime': 804.0906, 'train_samples_per_second': 38.313, 'train_steps_per_second': 4.791, 'total_flos': 4158476678393856.0, 'train_loss': 1.6634717696923704, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='161' max='161' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [161/161 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Model Accuracy: 0.2578\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics  \n",
    ")\n",
    "\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "accuracy = eval_results[\"eval_accuracy\"]\n",
    "print(f\"Trained Model Accuracy: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
