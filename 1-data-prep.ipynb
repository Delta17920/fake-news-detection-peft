{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch  \n",
    "from datasets import load_dataset, Dataset \n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, get_scheduler, RobertaForSequenceClassification\n",
    "from peft import LoraConfig, get_peft_model, AutoPeftModelForSequenceClassification  \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import evaluate\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context'],\n",
      "        num_rows: 10269\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context'],\n",
      "        num_rows: 1283\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context'],\n",
      "        num_rows: 1284\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "datasets = load_dataset(\"liar\",trust_remote_code=True)\n",
    "\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>statement</th>\n",
       "      <th>subject</th>\n",
       "      <th>speaker</th>\n",
       "      <th>job_title</th>\n",
       "      <th>state_info</th>\n",
       "      <th>party_affiliation</th>\n",
       "      <th>barely_true_counts</th>\n",
       "      <th>false_counts</th>\n",
       "      <th>half_true_counts</th>\n",
       "      <th>mostly_true_counts</th>\n",
       "      <th>pants_on_fire_counts</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3358.json</td>\n",
       "      <td>1</td>\n",
       "      <td>Thirty-five states have accepted high-speed in...</td>\n",
       "      <td>stimulus,transportation</td>\n",
       "      <td>raymond-lahood</td>\n",
       "      <td>Secretary, U.S. Department of Transportation</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a speech to the American Association of State ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8048.json</td>\n",
       "      <td>2</td>\n",
       "      <td>Since I was elected, crime rates have been at ...</td>\n",
       "      <td>crime</td>\n",
       "      <td>bill-foster</td>\n",
       "      <td>Mayor, St. Petersburg</td>\n",
       "      <td>Florida</td>\n",
       "      <td>republican</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a campaign brochure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8280.json</td>\n",
       "      <td>5</td>\n",
       "      <td>Warren Buffett recently said Scrap Obamacare a...</td>\n",
       "      <td>health-care,pundits</td>\n",
       "      <td>blog-posting</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>none</td>\n",
       "      <td>7.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>posts on the Internet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>481.json</td>\n",
       "      <td>2</td>\n",
       "      <td>Eliminating earmarks \"would make barely a drop...</td>\n",
       "      <td>federal-budget</td>\n",
       "      <td>bob-barr</td>\n",
       "      <td>Runs a consulting firm, Liberty Strategies LLC</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>libertarian</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a news conference announcing his Libertarian c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12651.json</td>\n",
       "      <td>4</td>\n",
       "      <td>Rather than work to secure the border, (Marco ...</td>\n",
       "      <td>immigration</td>\n",
       "      <td>carlos-beruff</td>\n",
       "      <td>Developer</td>\n",
       "      <td>Florida</td>\n",
       "      <td>republican</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a statement to press</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  label                                          statement  \\\n",
       "0   3358.json      1  Thirty-five states have accepted high-speed in...   \n",
       "1   8048.json      2  Since I was elected, crime rates have been at ...   \n",
       "2   8280.json      5  Warren Buffett recently said Scrap Obamacare a...   \n",
       "3    481.json      2  Eliminating earmarks \"would make barely a drop...   \n",
       "4  12651.json      4  Rather than work to secure the border, (Marco ...   \n",
       "\n",
       "                   subject         speaker  \\\n",
       "0  stimulus,transportation  raymond-lahood   \n",
       "1                    crime     bill-foster   \n",
       "2      health-care,pundits    blog-posting   \n",
       "3           federal-budget        bob-barr   \n",
       "4              immigration   carlos-beruff   \n",
       "\n",
       "                                        job_title state_info  \\\n",
       "0    Secretary, U.S. Department of Transportation   Illinois   \n",
       "1                           Mayor, St. Petersburg    Florida   \n",
       "2                                                              \n",
       "3  Runs a consulting firm, Liberty Strategies LLC    Georgia   \n",
       "4                                       Developer    Florida   \n",
       "\n",
       "  party_affiliation  barely_true_counts  false_counts  half_true_counts  \\\n",
       "0        republican                 0.0           1.0               1.0   \n",
       "1        republican                 1.0           0.0               2.0   \n",
       "2              none                 7.0          19.0               3.0   \n",
       "3       libertarian                 0.0           0.0               0.0   \n",
       "4        republican                 3.0           1.0               0.0   \n",
       "\n",
       "   mostly_true_counts  pants_on_fire_counts  \\\n",
       "0                 1.0                   0.0   \n",
       "1                 2.0                   0.0   \n",
       "2                 5.0                  44.0   \n",
       "3                 1.0                   0.0   \n",
       "4                 0.0                   0.0   \n",
       "\n",
       "                                             context  \n",
       "0  a speech to the American Association of State ...  \n",
       "1                                a campaign brochure  \n",
       "2                              posts on the Internet  \n",
       "3  a news conference announcing his Libertarian c...  \n",
       "4                               a statement to press  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff=pd.DataFrame(datasets[\"train\"].shuffle(seed=42).select(range(5)))\n",
    "dff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = ['id', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', \n",
    "                     'barely_true_counts', 'false_counts', 'half_true_counts', \n",
    "                     'mostly_true_counts', 'pants_on_fire_counts', 'context']\n",
    "\n",
    "dataset = datasets.remove_columns(columns_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CustomRobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_ck=\"roberta-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ck)\n",
    "\n",
    "\n",
    "class CustomRobertaModel(RobertaForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.dropout = torch.nn.Dropout(0.3)  # Increased dropout rate\n",
    "\n",
    "model = CustomRobertaModel.from_pretrained(model_ck, num_labels=6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fun(example):\n",
    "    return tokenizer(example[\"statement\"],padding=\"max_length\",truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_fun,batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Liardataset(Dataset):\n",
    "    def __init__(self,tokenized_data):\n",
    "        self.data = tokenized_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[\"input_ids\"])\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return {\n",
    "            \"input_ids\" : torch.tensor(self.data[\"input_ids\"][idx],dtype = torch.long),\n",
    "            \"attention_mask\": torch.tensor(self.data[\"attention_mask\"][idx], dtype=torch.long),\n",
    "            \"labels\": torch.tensor(self.data[\"label\"][idx], dtype=torch.long),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,  1620,  3428,  ...,     1,     1,     1],\n",
      "        [    0,  4030,    90,  ...,     1,     1,     1],\n",
      "        [    0, 11321,  1699,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,  4148,    36,  ...,     1,     1,     1],\n",
      "        [    0,   104,  4113,  ...,     1,     1,     1],\n",
      "        [    0,   133,  6866,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([5, 3, 4, 3, 1, 0, 5, 4, 1, 0, 3, 0, 3, 0, 5, 0])}\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_dataset = Liardataset(tokenized_dataset[\"train\"])\n",
    "val_dataset = Liardataset(tokenized_dataset[\"validation\"])\n",
    "test_dataset = Liardataset(tokenized_dataset[\"test\"])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset,batch_size=batch_size,shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False)\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "print(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained Model Accuracy: 0.1316\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        batch = {key:vals.to(device) for key,vals in batch.items()}\n",
    "        outputs = model(input_ids = batch[\"input_ids\"], attention_mask = batch[\"attention_mask\"])\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits,dim=-1)\n",
    "\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "accuracy = metric.compute()\n",
    "print(f\"Pretrained Model Accuracy: {accuracy['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,364,678 || all params: 127,014,924 || trainable%: 1.8617\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules = [\"query\", \"key\", \"value\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "if hasattr(model,\"lora_config\"):\n",
    "    model=model.unload()\n",
    "if hasattr(model,\"lora_config\"):\n",
    "    model=model.unload()\n",
    "peft_model = get_peft_model(model,lora_config)\n",
    "\n",
    "peft_model.to(device)\n",
    "\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.embeddings.word_embeddings.weight\n",
      "roberta.embeddings.position_embeddings.weight\n",
      "roberta.embeddings.token_type_embeddings.weight\n",
      "roberta.embeddings.LayerNorm.weight\n",
      "roberta.embeddings.LayerNorm.bias\n",
      "roberta.encoder.layer.0.attention.self.query.base_layer.weight\n",
      "roberta.encoder.layer.0.attention.self.query.base_layer.bias\n",
      "roberta.encoder.layer.0.attention.self.query.lora_A.default.weight\n",
      "roberta.encoder.layer.0.attention.self.query.lora_B.default.weight\n",
      "roberta.encoder.layer.0.attention.self.key.base_layer.weight\n",
      "roberta.encoder.layer.0.attention.self.key.base_layer.bias\n",
      "roberta.encoder.layer.0.attention.self.key.lora_A.default.weight\n",
      "roberta.encoder.layer.0.attention.self.key.lora_B.default.weight\n",
      "roberta.encoder.layer.0.attention.self.value.base_layer.weight\n",
      "roberta.encoder.layer.0.attention.self.value.base_layer.bias\n",
      "roberta.encoder.layer.0.attention.self.value.lora_A.default.weight\n",
      "roberta.encoder.layer.0.attention.self.value.lora_B.default.weight\n",
      "roberta.encoder.layer.0.attention.output.dense.weight\n",
      "roberta.encoder.layer.0.attention.output.dense.bias\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.0.intermediate.dense.weight\n",
      "roberta.encoder.layer.0.intermediate.dense.bias\n",
      "roberta.encoder.layer.0.output.dense.weight\n",
      "roberta.encoder.layer.0.output.dense.bias\n",
      "roberta.encoder.layer.0.output.LayerNorm.weight\n",
      "roberta.encoder.layer.0.output.LayerNorm.bias\n",
      "roberta.encoder.layer.1.attention.self.query.base_layer.weight\n",
      "roberta.encoder.layer.1.attention.self.query.base_layer.bias\n",
      "roberta.encoder.layer.1.attention.self.query.lora_A.default.weight\n",
      "roberta.encoder.layer.1.attention.self.query.lora_B.default.weight\n",
      "roberta.encoder.layer.1.attention.self.key.base_layer.weight\n",
      "roberta.encoder.layer.1.attention.self.key.base_layer.bias\n",
      "roberta.encoder.layer.1.attention.self.key.lora_A.default.weight\n",
      "roberta.encoder.layer.1.attention.self.key.lora_B.default.weight\n",
      "roberta.encoder.layer.1.attention.self.value.base_layer.weight\n",
      "roberta.encoder.layer.1.attention.self.value.base_layer.bias\n",
      "roberta.encoder.layer.1.attention.self.value.lora_A.default.weight\n",
      "roberta.encoder.layer.1.attention.self.value.lora_B.default.weight\n",
      "roberta.encoder.layer.1.attention.output.dense.weight\n",
      "roberta.encoder.layer.1.attention.output.dense.bias\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.1.intermediate.dense.weight\n",
      "roberta.encoder.layer.1.intermediate.dense.bias\n",
      "roberta.encoder.layer.1.output.dense.weight\n",
      "roberta.encoder.layer.1.output.dense.bias\n",
      "roberta.encoder.layer.1.output.LayerNorm.weight\n",
      "roberta.encoder.layer.1.output.LayerNorm.bias\n",
      "roberta.encoder.layer.2.attention.self.query.base_layer.weight\n",
      "roberta.encoder.layer.2.attention.self.query.base_layer.bias\n",
      "roberta.encoder.layer.2.attention.self.query.lora_A.default.weight\n",
      "roberta.encoder.layer.2.attention.self.query.lora_B.default.weight\n",
      "roberta.encoder.layer.2.attention.self.key.base_layer.weight\n",
      "roberta.encoder.layer.2.attention.self.key.base_layer.bias\n",
      "roberta.encoder.layer.2.attention.self.key.lora_A.default.weight\n",
      "roberta.encoder.layer.2.attention.self.key.lora_B.default.weight\n",
      "roberta.encoder.layer.2.attention.self.value.base_layer.weight\n",
      "roberta.encoder.layer.2.attention.self.value.base_layer.bias\n",
      "roberta.encoder.layer.2.attention.self.value.lora_A.default.weight\n",
      "roberta.encoder.layer.2.attention.self.value.lora_B.default.weight\n",
      "roberta.encoder.layer.2.attention.output.dense.weight\n",
      "roberta.encoder.layer.2.attention.output.dense.bias\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.2.intermediate.dense.weight\n",
      "roberta.encoder.layer.2.intermediate.dense.bias\n",
      "roberta.encoder.layer.2.output.dense.weight\n",
      "roberta.encoder.layer.2.output.dense.bias\n",
      "roberta.encoder.layer.2.output.LayerNorm.weight\n",
      "roberta.encoder.layer.2.output.LayerNorm.bias\n",
      "roberta.encoder.layer.3.attention.self.query.base_layer.weight\n",
      "roberta.encoder.layer.3.attention.self.query.base_layer.bias\n",
      "roberta.encoder.layer.3.attention.self.query.lora_A.default.weight\n",
      "roberta.encoder.layer.3.attention.self.query.lora_B.default.weight\n",
      "roberta.encoder.layer.3.attention.self.key.base_layer.weight\n",
      "roberta.encoder.layer.3.attention.self.key.base_layer.bias\n",
      "roberta.encoder.layer.3.attention.self.key.lora_A.default.weight\n",
      "roberta.encoder.layer.3.attention.self.key.lora_B.default.weight\n",
      "roberta.encoder.layer.3.attention.self.value.base_layer.weight\n",
      "roberta.encoder.layer.3.attention.self.value.base_layer.bias\n",
      "roberta.encoder.layer.3.attention.self.value.lora_A.default.weight\n",
      "roberta.encoder.layer.3.attention.self.value.lora_B.default.weight\n",
      "roberta.encoder.layer.3.attention.output.dense.weight\n",
      "roberta.encoder.layer.3.attention.output.dense.bias\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.3.intermediate.dense.weight\n",
      "roberta.encoder.layer.3.intermediate.dense.bias\n",
      "roberta.encoder.layer.3.output.dense.weight\n",
      "roberta.encoder.layer.3.output.dense.bias\n",
      "roberta.encoder.layer.3.output.LayerNorm.weight\n",
      "roberta.encoder.layer.3.output.LayerNorm.bias\n",
      "roberta.encoder.layer.4.attention.self.query.base_layer.weight\n",
      "roberta.encoder.layer.4.attention.self.query.base_layer.bias\n",
      "roberta.encoder.layer.4.attention.self.query.lora_A.default.weight\n",
      "roberta.encoder.layer.4.attention.self.query.lora_B.default.weight\n",
      "roberta.encoder.layer.4.attention.self.key.base_layer.weight\n",
      "roberta.encoder.layer.4.attention.self.key.base_layer.bias\n",
      "roberta.encoder.layer.4.attention.self.key.lora_A.default.weight\n",
      "roberta.encoder.layer.4.attention.self.key.lora_B.default.weight\n",
      "roberta.encoder.layer.4.attention.self.value.base_layer.weight\n",
      "roberta.encoder.layer.4.attention.self.value.base_layer.bias\n",
      "roberta.encoder.layer.4.attention.self.value.lora_A.default.weight\n",
      "roberta.encoder.layer.4.attention.self.value.lora_B.default.weight\n",
      "roberta.encoder.layer.4.attention.output.dense.weight\n",
      "roberta.encoder.layer.4.attention.output.dense.bias\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.4.intermediate.dense.weight\n",
      "roberta.encoder.layer.4.intermediate.dense.bias\n",
      "roberta.encoder.layer.4.output.dense.weight\n",
      "roberta.encoder.layer.4.output.dense.bias\n",
      "roberta.encoder.layer.4.output.LayerNorm.weight\n",
      "roberta.encoder.layer.4.output.LayerNorm.bias\n",
      "roberta.encoder.layer.5.attention.self.query.base_layer.weight\n",
      "roberta.encoder.layer.5.attention.self.query.base_layer.bias\n",
      "roberta.encoder.layer.5.attention.self.query.lora_A.default.weight\n",
      "roberta.encoder.layer.5.attention.self.query.lora_B.default.weight\n",
      "roberta.encoder.layer.5.attention.self.key.base_layer.weight\n",
      "roberta.encoder.layer.5.attention.self.key.base_layer.bias\n",
      "roberta.encoder.layer.5.attention.self.key.lora_A.default.weight\n",
      "roberta.encoder.layer.5.attention.self.key.lora_B.default.weight\n",
      "roberta.encoder.layer.5.attention.self.value.base_layer.weight\n",
      "roberta.encoder.layer.5.attention.self.value.base_layer.bias\n",
      "roberta.encoder.layer.5.attention.self.value.lora_A.default.weight\n",
      "roberta.encoder.layer.5.attention.self.value.lora_B.default.weight\n",
      "roberta.encoder.layer.5.attention.output.dense.weight\n",
      "roberta.encoder.layer.5.attention.output.dense.bias\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.5.intermediate.dense.weight\n",
      "roberta.encoder.layer.5.intermediate.dense.bias\n",
      "roberta.encoder.layer.5.output.dense.weight\n",
      "roberta.encoder.layer.5.output.dense.bias\n",
      "roberta.encoder.layer.5.output.LayerNorm.weight\n",
      "roberta.encoder.layer.5.output.LayerNorm.bias\n",
      "roberta.encoder.layer.6.attention.self.query.base_layer.weight\n",
      "roberta.encoder.layer.6.attention.self.query.base_layer.bias\n",
      "roberta.encoder.layer.6.attention.self.query.lora_A.default.weight\n",
      "roberta.encoder.layer.6.attention.self.query.lora_B.default.weight\n",
      "roberta.encoder.layer.6.attention.self.key.base_layer.weight\n",
      "roberta.encoder.layer.6.attention.self.key.base_layer.bias\n",
      "roberta.encoder.layer.6.attention.self.key.lora_A.default.weight\n",
      "roberta.encoder.layer.6.attention.self.key.lora_B.default.weight\n",
      "roberta.encoder.layer.6.attention.self.value.base_layer.weight\n",
      "roberta.encoder.layer.6.attention.self.value.base_layer.bias\n",
      "roberta.encoder.layer.6.attention.self.value.lora_A.default.weight\n",
      "roberta.encoder.layer.6.attention.self.value.lora_B.default.weight\n",
      "roberta.encoder.layer.6.attention.output.dense.weight\n",
      "roberta.encoder.layer.6.attention.output.dense.bias\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.6.intermediate.dense.weight\n",
      "roberta.encoder.layer.6.intermediate.dense.bias\n",
      "roberta.encoder.layer.6.output.dense.weight\n",
      "roberta.encoder.layer.6.output.dense.bias\n",
      "roberta.encoder.layer.6.output.LayerNorm.weight\n",
      "roberta.encoder.layer.6.output.LayerNorm.bias\n",
      "roberta.encoder.layer.7.attention.self.query.base_layer.weight\n",
      "roberta.encoder.layer.7.attention.self.query.base_layer.bias\n",
      "roberta.encoder.layer.7.attention.self.query.lora_A.default.weight\n",
      "roberta.encoder.layer.7.attention.self.query.lora_B.default.weight\n",
      "roberta.encoder.layer.7.attention.self.key.base_layer.weight\n",
      "roberta.encoder.layer.7.attention.self.key.base_layer.bias\n",
      "roberta.encoder.layer.7.attention.self.key.lora_A.default.weight\n",
      "roberta.encoder.layer.7.attention.self.key.lora_B.default.weight\n",
      "roberta.encoder.layer.7.attention.self.value.base_layer.weight\n",
      "roberta.encoder.layer.7.attention.self.value.base_layer.bias\n",
      "roberta.encoder.layer.7.attention.self.value.lora_A.default.weight\n",
      "roberta.encoder.layer.7.attention.self.value.lora_B.default.weight\n",
      "roberta.encoder.layer.7.attention.output.dense.weight\n",
      "roberta.encoder.layer.7.attention.output.dense.bias\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.7.intermediate.dense.weight\n",
      "roberta.encoder.layer.7.intermediate.dense.bias\n",
      "roberta.encoder.layer.7.output.dense.weight\n",
      "roberta.encoder.layer.7.output.dense.bias\n",
      "roberta.encoder.layer.7.output.LayerNorm.weight\n",
      "roberta.encoder.layer.7.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.attention.self.query.base_layer.weight\n",
      "roberta.encoder.layer.8.attention.self.query.base_layer.bias\n",
      "roberta.encoder.layer.8.attention.self.query.lora_A.default.weight\n",
      "roberta.encoder.layer.8.attention.self.query.lora_B.default.weight\n",
      "roberta.encoder.layer.8.attention.self.key.base_layer.weight\n",
      "roberta.encoder.layer.8.attention.self.key.base_layer.bias\n",
      "roberta.encoder.layer.8.attention.self.key.lora_A.default.weight\n",
      "roberta.encoder.layer.8.attention.self.key.lora_B.default.weight\n",
      "roberta.encoder.layer.8.attention.self.value.base_layer.weight\n",
      "roberta.encoder.layer.8.attention.self.value.base_layer.bias\n",
      "roberta.encoder.layer.8.attention.self.value.lora_A.default.weight\n",
      "roberta.encoder.layer.8.attention.self.value.lora_B.default.weight\n",
      "roberta.encoder.layer.8.attention.output.dense.weight\n",
      "roberta.encoder.layer.8.attention.output.dense.bias\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.intermediate.dense.weight\n",
      "roberta.encoder.layer.8.intermediate.dense.bias\n",
      "roberta.encoder.layer.8.output.dense.weight\n",
      "roberta.encoder.layer.8.output.dense.bias\n",
      "roberta.encoder.layer.8.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.attention.self.query.base_layer.weight\n",
      "roberta.encoder.layer.9.attention.self.query.base_layer.bias\n",
      "roberta.encoder.layer.9.attention.self.query.lora_A.default.weight\n",
      "roberta.encoder.layer.9.attention.self.query.lora_B.default.weight\n",
      "roberta.encoder.layer.9.attention.self.key.base_layer.weight\n",
      "roberta.encoder.layer.9.attention.self.key.base_layer.bias\n",
      "roberta.encoder.layer.9.attention.self.key.lora_A.default.weight\n",
      "roberta.encoder.layer.9.attention.self.key.lora_B.default.weight\n",
      "roberta.encoder.layer.9.attention.self.value.base_layer.weight\n",
      "roberta.encoder.layer.9.attention.self.value.base_layer.bias\n",
      "roberta.encoder.layer.9.attention.self.value.lora_A.default.weight\n",
      "roberta.encoder.layer.9.attention.self.value.lora_B.default.weight\n",
      "roberta.encoder.layer.9.attention.output.dense.weight\n",
      "roberta.encoder.layer.9.attention.output.dense.bias\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.intermediate.dense.weight\n",
      "roberta.encoder.layer.9.intermediate.dense.bias\n",
      "roberta.encoder.layer.9.output.dense.weight\n",
      "roberta.encoder.layer.9.output.dense.bias\n",
      "roberta.encoder.layer.9.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.attention.self.query.base_layer.weight\n",
      "roberta.encoder.layer.10.attention.self.query.base_layer.bias\n",
      "roberta.encoder.layer.10.attention.self.query.lora_A.default.weight\n",
      "roberta.encoder.layer.10.attention.self.query.lora_B.default.weight\n",
      "roberta.encoder.layer.10.attention.self.key.base_layer.weight\n",
      "roberta.encoder.layer.10.attention.self.key.base_layer.bias\n",
      "roberta.encoder.layer.10.attention.self.key.lora_A.default.weight\n",
      "roberta.encoder.layer.10.attention.self.key.lora_B.default.weight\n",
      "roberta.encoder.layer.10.attention.self.value.base_layer.weight\n",
      "roberta.encoder.layer.10.attention.self.value.base_layer.bias\n",
      "roberta.encoder.layer.10.attention.self.value.lora_A.default.weight\n",
      "roberta.encoder.layer.10.attention.self.value.lora_B.default.weight\n",
      "roberta.encoder.layer.10.attention.output.dense.weight\n",
      "roberta.encoder.layer.10.attention.output.dense.bias\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.intermediate.dense.weight\n",
      "roberta.encoder.layer.10.intermediate.dense.bias\n",
      "roberta.encoder.layer.10.output.dense.weight\n",
      "roberta.encoder.layer.10.output.dense.bias\n",
      "roberta.encoder.layer.10.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.attention.self.query.base_layer.weight\n",
      "roberta.encoder.layer.11.attention.self.query.base_layer.bias\n",
      "roberta.encoder.layer.11.attention.self.query.lora_A.default.weight\n",
      "roberta.encoder.layer.11.attention.self.query.lora_B.default.weight\n",
      "roberta.encoder.layer.11.attention.self.key.base_layer.weight\n",
      "roberta.encoder.layer.11.attention.self.key.base_layer.bias\n",
      "roberta.encoder.layer.11.attention.self.key.lora_A.default.weight\n",
      "roberta.encoder.layer.11.attention.self.key.lora_B.default.weight\n",
      "roberta.encoder.layer.11.attention.self.value.base_layer.weight\n",
      "roberta.encoder.layer.11.attention.self.value.base_layer.bias\n",
      "roberta.encoder.layer.11.attention.self.value.lora_A.default.weight\n",
      "roberta.encoder.layer.11.attention.self.value.lora_B.default.weight\n",
      "roberta.encoder.layer.11.attention.output.dense.weight\n",
      "roberta.encoder.layer.11.attention.output.dense.bias\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.intermediate.dense.weight\n",
      "roberta.encoder.layer.11.intermediate.dense.bias\n",
      "roberta.encoder.layer.11.output.dense.weight\n",
      "roberta.encoder.layer.11.output.dense.bias\n",
      "roberta.encoder.layer.11.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.output.LayerNorm.bias\n",
      "classifier.original_module.dense.weight\n",
      "classifier.original_module.dense.bias\n",
      "classifier.original_module.out_proj.weight\n",
      "classifier.original_module.out_proj.bias\n",
      "classifier.modules_to_save.default.dense.weight\n",
      "classifier.modules_to_save.default.dense.bias\n",
      "classifier.modules_to_save.default.out_proj.weight\n",
      "classifier.modules_to_save.default.out_proj.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "from transformers import Trainer\n",
    "\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        probs = probs.gather(dim=-1, index=targets.unsqueeze(-1))\n",
    "        probs = probs.squeeze(-1)\n",
    "        loss = -((1 - probs) ** self.gamma) * torch.log(probs)\n",
    "        return loss.mean()\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fn = FocalLoss(gamma=2.0)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5136' max='5136' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5136/5136 56:52, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.193600</td>\n",
       "      <td>1.209208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.190400</td>\n",
       "      <td>1.200664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.184600</td>\n",
       "      <td>1.169435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.101800</td>\n",
       "      <td>1.158867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.146300</td>\n",
       "      <td>1.135755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.100400</td>\n",
       "      <td>1.125684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.108500</td>\n",
       "      <td>1.121720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.198100</td>\n",
       "      <td>1.133045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5136, training_loss=1.1567948608197898, metrics={'train_runtime': 3413.988, 'train_samples_per_second': 24.063, 'train_steps_per_second': 1.504, 'total_flos': 2.2212651732074496e+16, 'train_loss': 1.1567948608197898, 'epoch': 8.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=8,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    push_to_hub=False,\n",
    "    label_names=[\"labels\"],\n",
    "    optim=\"adamw_torch\",  \n",
    "    lr_scheduler_type=\"cosine\"  \n",
    ")\n",
    "\n",
    "num_training_steps = len(train_dataloader) * training_args.num_train_epochs\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"cosine\",\n",
    "    optimizer=AdamW(peft_model.parameters(), lr=training_args.learning_rate),\n",
    "    num_warmup_steps=int(0.1 * num_training_steps),\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    optimizers=(AdamW(peft_model.parameters(), lr=training_args.learning_rate), lr_scheduler),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='81' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [81/81 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Model Accuracy: 0.2671\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics  \n",
    ")\n",
    "\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "accuracy = eval_results[\"eval_accuracy\"]\n",
    "print(f\"Trained Model Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'peft_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftModel\n\u001b[1;32m----> 3\u001b[0m \u001b[43mpeft_model\u001b[49m\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeft_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'peft_model' is not defined"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "peft_model.save_pretrained(\"peft_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\",num_labels=6)\n",
    "lora_model = PeftModel.from_pretrained(base_model, \"peft_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'PeftModelForSequenceClassification' is not supported for text-classification. Supported models are ['AlbertForSequenceClassification', 'BartForSequenceClassification', 'BertForSequenceClassification', 'BigBirdForSequenceClassification', 'BigBirdPegasusForSequenceClassification', 'BioGptForSequenceClassification', 'BloomForSequenceClassification', 'CamembertForSequenceClassification', 'CanineForSequenceClassification', 'LlamaForSequenceClassification', 'ConvBertForSequenceClassification', 'CTRLForSequenceClassification', 'Data2VecTextForSequenceClassification', 'DebertaForSequenceClassification', 'DebertaV2ForSequenceClassification', 'DiffLlamaForSequenceClassification', 'DistilBertForSequenceClassification', 'ElectraForSequenceClassification', 'ErnieForSequenceClassification', 'ErnieMForSequenceClassification', 'EsmForSequenceClassification', 'FalconForSequenceClassification', 'FlaubertForSequenceClassification', 'FNetForSequenceClassification', 'FunnelForSequenceClassification', 'GemmaForSequenceClassification', 'Gemma2ForSequenceClassification', 'GlmForSequenceClassification', 'GPT2ForSequenceClassification', 'GPT2ForSequenceClassification', 'GPTBigCodeForSequenceClassification', 'GPTNeoForSequenceClassification', 'GPTNeoXForSequenceClassification', 'GPTJForSequenceClassification', 'HeliumForSequenceClassification', 'IBertForSequenceClassification', 'JambaForSequenceClassification', 'JetMoeForSequenceClassification', 'LayoutLMForSequenceClassification', 'LayoutLMv2ForSequenceClassification', 'LayoutLMv3ForSequenceClassification', 'LEDForSequenceClassification', 'LiltForSequenceClassification', 'LlamaForSequenceClassification', 'LongformerForSequenceClassification', 'LukeForSequenceClassification', 'MarkupLMForSequenceClassification', 'MBartForSequenceClassification', 'MegaForSequenceClassification', 'MegatronBertForSequenceClassification', 'MistralForSequenceClassification', 'MixtralForSequenceClassification', 'MobileBertForSequenceClassification', 'ModernBertForSequenceClassification', 'MPNetForSequenceClassification', 'MptForSequenceClassification', 'MraForSequenceClassification', 'MT5ForSequenceClassification', 'MvpForSequenceClassification', 'NemotronForSequenceClassification', 'NezhaForSequenceClassification', 'NystromformerForSequenceClassification', 'OpenLlamaForSequenceClassification', 'OpenAIGPTForSequenceClassification', 'OPTForSequenceClassification', 'PerceiverForSequenceClassification', 'PersimmonForSequenceClassification', 'PhiForSequenceClassification', 'Phi3ForSequenceClassification', 'PhimoeForSequenceClassification', 'PLBartForSequenceClassification', 'QDQBertForSequenceClassification', 'Qwen2ForSequenceClassification', 'Qwen2MoeForSequenceClassification', 'ReformerForSequenceClassification', 'RemBertForSequenceClassification', 'RobertaForSequenceClassification', 'RobertaPreLayerNormForSequenceClassification', 'RoCBertForSequenceClassification', 'RoFormerForSequenceClassification', 'SqueezeBertForSequenceClassification', 'StableLmForSequenceClassification', 'Starcoder2ForSequenceClassification', 'T5ForSequenceClassification', 'TapasForSequenceClassification', 'TransfoXLForSequenceClassification', 'UMT5ForSequenceClassification', 'XLMForSequenceClassification', 'XLMRobertaForSequenceClassification', 'XLMRobertaXLForSequenceClassification', 'XLNetForSequenceClassification', 'XmodForSequenceClassification', 'YosoForSequenceClassification', 'ZambaForSequenceClassification', 'Zamba2ForSequenceClassification'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: True (Confidence: 0.25)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline  \n",
    "id2label = {\n",
    "    0: \"True\",\n",
    "    1: \"Mostly-True\",\n",
    "    2: \"Half-True\",\n",
    "    3: \"Barely-True\",\n",
    "    4: \"False\",\n",
    "    5: \"Pants-on-Fire\"\n",
    "}\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=lora_model, tokenizer=tokenizer)  \n",
    "\n",
    "text = \"I will become the new AIML Lead\"\n",
    "result = classifier(text)[0]  \n",
    "predicted_label = id2label[int(result['label'].split('_')[-1])]  \n",
    "confidence = result['score']\n",
    "\n",
    "print(f\"Predicted Label: {predicted_label} (Confidence: {confidence:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'PeftModelForSequenceClassification' is not supported for text-classification. Supported models are ['AlbertForSequenceClassification', 'BartForSequenceClassification', 'BertForSequenceClassification', 'BigBirdForSequenceClassification', 'BigBirdPegasusForSequenceClassification', 'BioGptForSequenceClassification', 'BloomForSequenceClassification', 'CamembertForSequenceClassification', 'CanineForSequenceClassification', 'LlamaForSequenceClassification', 'ConvBertForSequenceClassification', 'CTRLForSequenceClassification', 'Data2VecTextForSequenceClassification', 'DebertaForSequenceClassification', 'DebertaV2ForSequenceClassification', 'DiffLlamaForSequenceClassification', 'DistilBertForSequenceClassification', 'ElectraForSequenceClassification', 'ErnieForSequenceClassification', 'ErnieMForSequenceClassification', 'EsmForSequenceClassification', 'FalconForSequenceClassification', 'FlaubertForSequenceClassification', 'FNetForSequenceClassification', 'FunnelForSequenceClassification', 'GemmaForSequenceClassification', 'Gemma2ForSequenceClassification', 'GlmForSequenceClassification', 'GPT2ForSequenceClassification', 'GPT2ForSequenceClassification', 'GPTBigCodeForSequenceClassification', 'GPTNeoForSequenceClassification', 'GPTNeoXForSequenceClassification', 'GPTJForSequenceClassification', 'HeliumForSequenceClassification', 'IBertForSequenceClassification', 'JambaForSequenceClassification', 'JetMoeForSequenceClassification', 'LayoutLMForSequenceClassification', 'LayoutLMv2ForSequenceClassification', 'LayoutLMv3ForSequenceClassification', 'LEDForSequenceClassification', 'LiltForSequenceClassification', 'LlamaForSequenceClassification', 'LongformerForSequenceClassification', 'LukeForSequenceClassification', 'MarkupLMForSequenceClassification', 'MBartForSequenceClassification', 'MegaForSequenceClassification', 'MegatronBertForSequenceClassification', 'MistralForSequenceClassification', 'MixtralForSequenceClassification', 'MobileBertForSequenceClassification', 'ModernBertForSequenceClassification', 'MPNetForSequenceClassification', 'MptForSequenceClassification', 'MraForSequenceClassification', 'MT5ForSequenceClassification', 'MvpForSequenceClassification', 'NemotronForSequenceClassification', 'NezhaForSequenceClassification', 'NystromformerForSequenceClassification', 'OpenLlamaForSequenceClassification', 'OpenAIGPTForSequenceClassification', 'OPTForSequenceClassification', 'PerceiverForSequenceClassification', 'PersimmonForSequenceClassification', 'PhiForSequenceClassification', 'Phi3ForSequenceClassification', 'PhimoeForSequenceClassification', 'PLBartForSequenceClassification', 'QDQBertForSequenceClassification', 'Qwen2ForSequenceClassification', 'Qwen2MoeForSequenceClassification', 'ReformerForSequenceClassification', 'RemBertForSequenceClassification', 'RobertaForSequenceClassification', 'RobertaPreLayerNormForSequenceClassification', 'RoCBertForSequenceClassification', 'RoFormerForSequenceClassification', 'SqueezeBertForSequenceClassification', 'StableLmForSequenceClassification', 'Starcoder2ForSequenceClassification', 'T5ForSequenceClassification', 'TapasForSequenceClassification', 'TransfoXLForSequenceClassification', 'UMT5ForSequenceClassification', 'XLMForSequenceClassification', 'XLMRobertaForSequenceClassification', 'XLMRobertaXLForSequenceClassification', 'XLNetForSequenceClassification', 'XmodForSequenceClassification', 'YosoForSequenceClassification', 'ZambaForSequenceClassification', 'Zamba2ForSequenceClassification'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: True (Confidence: 0.28)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline  \n",
    "id2label = {\n",
    "    0: \"True\",\n",
    "    1: \"Mostly-True\",\n",
    "    2: \"Half-True\",\n",
    "    3: \"Barely-True\",\n",
    "    4: \"False\",\n",
    "    5: \"Pants-on-Fire\"\n",
    "}\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=lora_model, tokenizer=tokenizer)  \n",
    "\n",
    "text = \"I will not become the new AIML Lead\"\n",
    "result = classifier(text)[0]  \n",
    "predicted_label = id2label[int(result['label'].split('_')[-1])]  \n",
    "confidence = result['score']\n",
    "\n",
    "print(f\"Predicted Label: {predicted_label} (Confidence: {confidence:.2f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
