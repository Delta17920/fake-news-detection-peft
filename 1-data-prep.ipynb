{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Imports:\n",
    "\n",
    "import torch  \n",
    "from datasets import load_dataset, Dataset \n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, get_scheduler, RobertaForSequenceClassification\n",
    "from peft import LoraConfig, get_peft_model, AutoPeftModelForSequenceClassification  \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import evaluate\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context'],\n",
      "        num_rows: 10269\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context'],\n",
      "        num_rows: 1283\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context'],\n",
      "        num_rows: 1284\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "datasets = load_dataset(\"liar\",trust_remote_code=True)\n",
    "\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>statement</th>\n",
       "      <th>subject</th>\n",
       "      <th>speaker</th>\n",
       "      <th>job_title</th>\n",
       "      <th>state_info</th>\n",
       "      <th>party_affiliation</th>\n",
       "      <th>barely_true_counts</th>\n",
       "      <th>false_counts</th>\n",
       "      <th>half_true_counts</th>\n",
       "      <th>mostly_true_counts</th>\n",
       "      <th>pants_on_fire_counts</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3358.json</td>\n",
       "      <td>1</td>\n",
       "      <td>Thirty-five states have accepted high-speed in...</td>\n",
       "      <td>stimulus,transportation</td>\n",
       "      <td>raymond-lahood</td>\n",
       "      <td>Secretary, U.S. Department of Transportation</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a speech to the American Association of State ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8048.json</td>\n",
       "      <td>2</td>\n",
       "      <td>Since I was elected, crime rates have been at ...</td>\n",
       "      <td>crime</td>\n",
       "      <td>bill-foster</td>\n",
       "      <td>Mayor, St. Petersburg</td>\n",
       "      <td>Florida</td>\n",
       "      <td>republican</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a campaign brochure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8280.json</td>\n",
       "      <td>5</td>\n",
       "      <td>Warren Buffett recently said Scrap Obamacare a...</td>\n",
       "      <td>health-care,pundits</td>\n",
       "      <td>blog-posting</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>none</td>\n",
       "      <td>7.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>posts on the Internet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>481.json</td>\n",
       "      <td>2</td>\n",
       "      <td>Eliminating earmarks \"would make barely a drop...</td>\n",
       "      <td>federal-budget</td>\n",
       "      <td>bob-barr</td>\n",
       "      <td>Runs a consulting firm, Liberty Strategies LLC</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>libertarian</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a news conference announcing his Libertarian c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12651.json</td>\n",
       "      <td>4</td>\n",
       "      <td>Rather than work to secure the border, (Marco ...</td>\n",
       "      <td>immigration</td>\n",
       "      <td>carlos-beruff</td>\n",
       "      <td>Developer</td>\n",
       "      <td>Florida</td>\n",
       "      <td>republican</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a statement to press</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  label                                          statement  \\\n",
       "0   3358.json      1  Thirty-five states have accepted high-speed in...   \n",
       "1   8048.json      2  Since I was elected, crime rates have been at ...   \n",
       "2   8280.json      5  Warren Buffett recently said Scrap Obamacare a...   \n",
       "3    481.json      2  Eliminating earmarks \"would make barely a drop...   \n",
       "4  12651.json      4  Rather than work to secure the border, (Marco ...   \n",
       "\n",
       "                   subject         speaker  \\\n",
       "0  stimulus,transportation  raymond-lahood   \n",
       "1                    crime     bill-foster   \n",
       "2      health-care,pundits    blog-posting   \n",
       "3           federal-budget        bob-barr   \n",
       "4              immigration   carlos-beruff   \n",
       "\n",
       "                                        job_title state_info  \\\n",
       "0    Secretary, U.S. Department of Transportation   Illinois   \n",
       "1                           Mayor, St. Petersburg    Florida   \n",
       "2                                                              \n",
       "3  Runs a consulting firm, Liberty Strategies LLC    Georgia   \n",
       "4                                       Developer    Florida   \n",
       "\n",
       "  party_affiliation  barely_true_counts  false_counts  half_true_counts  \\\n",
       "0        republican                 0.0           1.0               1.0   \n",
       "1        republican                 1.0           0.0               2.0   \n",
       "2              none                 7.0          19.0               3.0   \n",
       "3       libertarian                 0.0           0.0               0.0   \n",
       "4        republican                 3.0           1.0               0.0   \n",
       "\n",
       "   mostly_true_counts  pants_on_fire_counts  \\\n",
       "0                 1.0                   0.0   \n",
       "1                 2.0                   0.0   \n",
       "2                 5.0                  44.0   \n",
       "3                 1.0                   0.0   \n",
       "4                 0.0                   0.0   \n",
       "\n",
       "                                             context  \n",
       "0  a speech to the American Association of State ...  \n",
       "1                                a campaign brochure  \n",
       "2                              posts on the Internet  \n",
       "3  a news conference announcing his Libertarian c...  \n",
       "4                               a statement to press  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff=pd.DataFrame(datasets[\"train\"].shuffle(seed=42).select(range(5)))\n",
    "dff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = ['id', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', \n",
    "                     'barely_true_counts', 'false_counts', 'half_true_counts', \n",
    "                     'mostly_true_counts', 'pants_on_fire_counts', 'context']\n",
    "\n",
    "dataset = datasets.remove_columns(columns_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808ab6537eab41179f6cf7fbbd95ef2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11a06997d6b645849c6fc01967d3033a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c03855912244b0817aada249140db6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "304d481f04d345699f5acaf300c68dfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f129e0fd95469e9e02136275d2f5bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc26e25aab4241cfa7819c8ecf0c83d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CustomRobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_ck=\"roberta-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ck)\n",
    "\n",
    "\n",
    "class CustomRobertaModel(RobertaForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.dropout = torch.nn.Dropout(0.3)  # Increased dropout rate\n",
    "\n",
    "model = CustomRobertaModel.from_pretrained(model_ck, num_labels=6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf50918fe3cb416abd0cb3da75b70308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10269 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fa44b8be3024b88baf9a8cb8647091f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1283 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3c3d43ed174b3d9110cf308619b997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1284 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_fun(example):\n",
    "    return tokenizer(example[\"statement\"],padding=\"max_length\",truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_fun,batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Liardataset(Dataset):\n",
    "    def __init__(self,tokenized_data):\n",
    "        self.data = tokenized_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[\"input_ids\"])\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return {\n",
    "            \"input_ids\" : torch.tensor(self.data[\"input_ids\"][idx],dtype = torch.long),\n",
    "            \"attention_mask\": torch.tensor(self.data[\"attention_mask\"][idx], dtype=torch.long),\n",
    "            \"labels\": torch.tensor(self.data[\"label\"][idx], dtype=torch.long),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   113,  2709,  ...,     1,     1,     1],\n",
      "        [    0,   104,  4113,  ...,     1,     1,     1],\n",
      "        [    0,   104,  4113,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0, 38067,  4636,  ...,     1,     1,     1],\n",
      "        [    0,   113, 19993,  ...,     1,     1,     1],\n",
      "        [    0,   717, 25529,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([4, 4, 2, 1, 0, 2, 0, 2, 1, 2, 0, 1, 3, 3, 2, 2])}\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_dataset = Liardataset(tokenized_dataset[\"train\"])\n",
    "val_dataset = Liardataset(tokenized_dataset[\"validation\"])\n",
    "test_dataset = Liardataset(tokenized_dataset[\"test\"])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset,batch_size=batch_size,shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False)\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "print(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained Model Accuracy: 0.2048\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        batch = {key:vals.to(device) for key,vals in batch.items()}\n",
    "        outputs = model(input_ids = batch[\"input_ids\"], attention_mask = batch[\"attention_mask\"])\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits,dim=-1)\n",
    "\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "accuracy = metric.compute()\n",
    "print(f\"Pretrained Model Accuracy: {accuracy['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,364,678 || all params: 127,014,924 || trainable%: 1.8617\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules = [\"query\", \"key\", \"value\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "if hasattr(model,\"lora_config\"):\n",
    "    model=model.unload()\n",
    "if hasattr(model,\"lora_config\"):\n",
    "    model=model.unload()\n",
    "peft_model = get_peft_model(model,lora_config)\n",
    "\n",
    "peft_model.to(device)\n",
    "\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.embeddings.word_embeddings.weight\n",
      "roberta.embeddings.position_embeddings.weight\n",
      "roberta.embeddings.token_type_embeddings.weight\n",
      "roberta.embeddings.LayerNorm.weight\n",
      "roberta.embeddings.LayerNorm.bias\n",
      "roberta.encoder.layer.0.attention.self.query.base_layer.weight\n",
      "roberta.encoder.layer.0.attention.self.query.base_layer.bias\n",
      "roberta.encoder.layer.0.attention.self.query.lora_A.default.weight\n",
      "roberta.encoder.layer.0.attention.self.query.lora_B.default.weight\n",
      "roberta.encoder.layer.0.attention.self.key.base_layer.weight\n",
      "roberta.encoder.layer.0.attention.self.key.base_layer.bias\n",
      "roberta.encoder.layer.0.attention.self.key.lora_A.default.weight\n",
      "roberta.encoder.layer.0.attention.self.key.lora_B.default.weight\n",
      "roberta.encoder.layer.0.attention.self.value.base_layer.weight\n",
      "roberta.encoder.layer.0.attention.self.value.base_layer.bias\n",
      "roberta.encoder.layer.0.attention.self.value.lora_A.default.weight\n",
      "roberta.encoder.layer.0.attention.self.value.lora_B.default.weight\n",
      "roberta.encoder.layer.0.attention.output.dense.weight\n",
      "roberta.encoder.layer.0.attention.output.dense.bias\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.0.intermediate.dense.weight\n",
      "roberta.encoder.layer.0.intermediate.dense.bias\n",
      "roberta.encoder.layer.0.output.dense.weight\n",
      "roberta.encoder.layer.0.output.dense.bias\n",
      "roberta.encoder.layer.0.output.LayerNorm.weight\n",
      "roberta.encoder.layer.0.output.LayerNorm.bias\n",
      "roberta.encoder.layer.1.attention.self.query.base_layer.weight\n",
      "roberta.encoder.layer.1.attention.self.query.base_layer.bias\n",
      "roberta.encoder.layer.1.attention.self.query.lora_A.default.weight\n",
      "roberta.encoder.layer.1.attention.self.query.lora_B.default.weight\n",
      "roberta.encoder.layer.1.attention.self.key.base_layer.weight\n",
      "roberta.encoder.layer.1.attention.self.key.base_layer.bias\n",
      "roberta.encoder.layer.1.attention.self.key.lora_A.default.weight\n",
      "roberta.encoder.layer.1.attention.self.key.lora_B.default.weight\n",
      "roberta.encoder.layer.1.attention.self.value.base_layer.weight\n",
      "roberta.encoder.layer.1.attention.self.value.base_layer.bias\n",
      "roberta.encoder.layer.1.attention.self.value.lora_A.default.weight\n",
      "roberta.encoder.layer.1.attention.self.value.lora_B.default.weight\n",
      "roberta.encoder.layer.1.attention.output.dense.weight\n",
      "roberta.encoder.layer.1.attention.output.dense.bias\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.1.intermediate.dense.weight\n",
      "roberta.encoder.layer.1.intermediate.dense.bias\n",
      "roberta.encoder.layer.1.output.dense.weight\n",
      "roberta.encoder.layer.1.output.dense.bias\n",
      "roberta.encoder.layer.1.output.LayerNorm.weight\n",
      "roberta.encoder.layer.1.output.LayerNorm.bias\n",
      "roberta.encoder.layer.2.attention.self.query.base_layer.weight\n",
      "roberta.encoder.layer.2.attention.self.query.base_layer.bias\n",
      "roberta.encoder.layer.2.attention.self.query.lora_A.default.weight\n",
      "roberta.encoder.layer.2.attention.self.query.lora_B.default.weight\n",
      "roberta.encoder.layer.2.attention.self.key.base_layer.weight\n",
      "roberta.encoder.layer.2.attention.self.key.base_layer.bias\n",
      "roberta.encoder.layer.2.attention.self.key.lora_A.default.weight\n",
      "roberta.encoder.layer.2.attention.self.key.lora_B.default.weight\n",
      "roberta.encoder.layer.2.attention.self.value.base_layer.weight\n",
      "roberta.encoder.layer.2.attention.self.value.base_layer.bias\n",
      "roberta.encoder.layer.2.attention.self.value.lora_A.default.weight\n",
      "roberta.encoder.layer.2.attention.self.value.lora_B.default.weight\n",
      "roberta.encoder.layer.2.attention.output.dense.weight\n",
      "roberta.encoder.layer.2.attention.output.dense.bias\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.2.intermediate.dense.weight\n",
      "roberta.encoder.layer.2.intermediate.dense.bias\n",
      "roberta.encoder.layer.2.output.dense.weight\n",
      "roberta.encoder.layer.2.output.dense.bias\n",
      "roberta.encoder.layer.2.output.LayerNorm.weight\n",
      "roberta.encoder.layer.2.output.LayerNorm.bias\n",
      "roberta.encoder.layer.3.attention.self.query.base_layer.weight\n",
      "roberta.encoder.layer.3.attention.self.query.base_layer.bias\n",
      "roberta.encoder.layer.3.attention.self.query.lora_A.default.weight\n",
      "roberta.encoder.layer.3.attention.self.query.lora_B.default.weight\n",
      "roberta.encoder.layer.3.attention.self.key.base_layer.weight\n",
      "roberta.encoder.layer.3.attention.self.key.base_layer.bias\n",
      "roberta.encoder.layer.3.attention.self.key.lora_A.default.weight\n",
      "roberta.encoder.layer.3.attention.self.key.lora_B.default.weight\n",
      "roberta.encoder.layer.3.attention.self.value.base_layer.weight\n",
      "roberta.encoder.layer.3.attention.self.value.base_layer.bias\n",
      "roberta.encoder.layer.3.attention.self.value.lora_A.default.weight\n",
      "roberta.encoder.layer.3.attention.self.value.lora_B.default.weight\n",
      "roberta.encoder.layer.3.attention.output.dense.weight\n",
      "roberta.encoder.layer.3.attention.output.dense.bias\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.3.intermediate.dense.weight\n",
      "roberta.encoder.layer.3.intermediate.dense.bias\n",
      "roberta.encoder.layer.3.output.dense.weight\n",
      "roberta.encoder.layer.3.output.dense.bias\n",
      "roberta.encoder.layer.3.output.LayerNorm.weight\n",
      "roberta.encoder.layer.3.output.LayerNorm.bias\n",
      "roberta.encoder.layer.4.attention.self.query.base_layer.weight\n",
      "roberta.encoder.layer.4.attention.self.query.base_layer.bias\n",
      "roberta.encoder.layer.4.attention.self.query.lora_A.default.weight\n",
      "roberta.encoder.layer.4.attention.self.query.lora_B.default.weight\n",
      "roberta.encoder.layer.4.attention.self.key.base_layer.weight\n",
      "roberta.encoder.layer.4.attention.self.key.base_layer.bias\n",
      "roberta.encoder.layer.4.attention.self.key.lora_A.default.weight\n",
      "roberta.encoder.layer.4.attention.self.key.lora_B.default.weight\n",
      "roberta.encoder.layer.4.attention.self.value.base_layer.weight\n",
      "roberta.encoder.layer.4.attention.self.value.base_layer.bias\n",
      "roberta.encoder.layer.4.attention.self.value.lora_A.default.weight\n",
      "roberta.encoder.layer.4.attention.self.value.lora_B.default.weight\n",
      "roberta.encoder.layer.4.attention.output.dense.weight\n",
      "roberta.encoder.layer.4.attention.output.dense.bias\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.4.intermediate.dense.weight\n",
      "roberta.encoder.layer.4.intermediate.dense.bias\n",
      "roberta.encoder.layer.4.output.dense.weight\n",
      "roberta.encoder.layer.4.output.dense.bias\n",
      "roberta.encoder.layer.4.output.LayerNorm.weight\n",
      "roberta.encoder.layer.4.output.LayerNorm.bias\n",
      "roberta.encoder.layer.5.attention.self.query.base_layer.weight\n",
      "roberta.encoder.layer.5.attention.self.query.base_layer.bias\n",
      "roberta.encoder.layer.5.attention.self.query.lora_A.default.weight\n",
      "roberta.encoder.layer.5.attention.self.query.lora_B.default.weight\n",
      "roberta.encoder.layer.5.attention.self.key.base_layer.weight\n",
      "roberta.encoder.layer.5.attention.self.key.base_layer.bias\n",
      "roberta.encoder.layer.5.attention.self.key.lora_A.default.weight\n",
      "roberta.encoder.layer.5.attention.self.key.lora_B.default.weight\n",
      "roberta.encoder.layer.5.attention.self.value.base_layer.weight\n",
      "roberta.encoder.layer.5.attention.self.value.base_layer.bias\n",
      "roberta.encoder.layer.5.attention.self.value.lora_A.default.weight\n",
      "roberta.encoder.layer.5.attention.self.value.lora_B.default.weight\n",
      "roberta.encoder.layer.5.attention.output.dense.weight\n",
      "roberta.encoder.layer.5.attention.output.dense.bias\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.5.intermediate.dense.weight\n",
      "roberta.encoder.layer.5.intermediate.dense.bias\n",
      "roberta.encoder.layer.5.output.dense.weight\n",
      "roberta.encoder.layer.5.output.dense.bias\n",
      "roberta.encoder.layer.5.output.LayerNorm.weight\n",
      "roberta.encoder.layer.5.output.LayerNorm.bias\n",
      "roberta.encoder.layer.6.attention.self.query.base_layer.weight\n",
      "roberta.encoder.layer.6.attention.self.query.base_layer.bias\n",
      "roberta.encoder.layer.6.attention.self.query.lora_A.default.weight\n",
      "roberta.encoder.layer.6.attention.self.query.lora_B.default.weight\n",
      "roberta.encoder.layer.6.attention.self.key.base_layer.weight\n",
      "roberta.encoder.layer.6.attention.self.key.base_layer.bias\n",
      "roberta.encoder.layer.6.attention.self.key.lora_A.default.weight\n",
      "roberta.encoder.layer.6.attention.self.key.lora_B.default.weight\n",
      "roberta.encoder.layer.6.attention.self.value.base_layer.weight\n",
      "roberta.encoder.layer.6.attention.self.value.base_layer.bias\n",
      "roberta.encoder.layer.6.attention.self.value.lora_A.default.weight\n",
      "roberta.encoder.layer.6.attention.self.value.lora_B.default.weight\n",
      "roberta.encoder.layer.6.attention.output.dense.weight\n",
      "roberta.encoder.layer.6.attention.output.dense.bias\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.6.intermediate.dense.weight\n",
      "roberta.encoder.layer.6.intermediate.dense.bias\n",
      "roberta.encoder.layer.6.output.dense.weight\n",
      "roberta.encoder.layer.6.output.dense.bias\n",
      "roberta.encoder.layer.6.output.LayerNorm.weight\n",
      "roberta.encoder.layer.6.output.LayerNorm.bias\n",
      "roberta.encoder.layer.7.attention.self.query.base_layer.weight\n",
      "roberta.encoder.layer.7.attention.self.query.base_layer.bias\n",
      "roberta.encoder.layer.7.attention.self.query.lora_A.default.weight\n",
      "roberta.encoder.layer.7.attention.self.query.lora_B.default.weight\n",
      "roberta.encoder.layer.7.attention.self.key.base_layer.weight\n",
      "roberta.encoder.layer.7.attention.self.key.base_layer.bias\n",
      "roberta.encoder.layer.7.attention.self.key.lora_A.default.weight\n",
      "roberta.encoder.layer.7.attention.self.key.lora_B.default.weight\n",
      "roberta.encoder.layer.7.attention.self.value.base_layer.weight\n",
      "roberta.encoder.layer.7.attention.self.value.base_layer.bias\n",
      "roberta.encoder.layer.7.attention.self.value.lora_A.default.weight\n",
      "roberta.encoder.layer.7.attention.self.value.lora_B.default.weight\n",
      "roberta.encoder.layer.7.attention.output.dense.weight\n",
      "roberta.encoder.layer.7.attention.output.dense.bias\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.7.intermediate.dense.weight\n",
      "roberta.encoder.layer.7.intermediate.dense.bias\n",
      "roberta.encoder.layer.7.output.dense.weight\n",
      "roberta.encoder.layer.7.output.dense.bias\n",
      "roberta.encoder.layer.7.output.LayerNorm.weight\n",
      "roberta.encoder.layer.7.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.attention.self.query.base_layer.weight\n",
      "roberta.encoder.layer.8.attention.self.query.base_layer.bias\n",
      "roberta.encoder.layer.8.attention.self.query.lora_A.default.weight\n",
      "roberta.encoder.layer.8.attention.self.query.lora_B.default.weight\n",
      "roberta.encoder.layer.8.attention.self.key.base_layer.weight\n",
      "roberta.encoder.layer.8.attention.self.key.base_layer.bias\n",
      "roberta.encoder.layer.8.attention.self.key.lora_A.default.weight\n",
      "roberta.encoder.layer.8.attention.self.key.lora_B.default.weight\n",
      "roberta.encoder.layer.8.attention.self.value.base_layer.weight\n",
      "roberta.encoder.layer.8.attention.self.value.base_layer.bias\n",
      "roberta.encoder.layer.8.attention.self.value.lora_A.default.weight\n",
      "roberta.encoder.layer.8.attention.self.value.lora_B.default.weight\n",
      "roberta.encoder.layer.8.attention.output.dense.weight\n",
      "roberta.encoder.layer.8.attention.output.dense.bias\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.intermediate.dense.weight\n",
      "roberta.encoder.layer.8.intermediate.dense.bias\n",
      "roberta.encoder.layer.8.output.dense.weight\n",
      "roberta.encoder.layer.8.output.dense.bias\n",
      "roberta.encoder.layer.8.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.attention.self.query.base_layer.weight\n",
      "roberta.encoder.layer.9.attention.self.query.base_layer.bias\n",
      "roberta.encoder.layer.9.attention.self.query.lora_A.default.weight\n",
      "roberta.encoder.layer.9.attention.self.query.lora_B.default.weight\n",
      "roberta.encoder.layer.9.attention.self.key.base_layer.weight\n",
      "roberta.encoder.layer.9.attention.self.key.base_layer.bias\n",
      "roberta.encoder.layer.9.attention.self.key.lora_A.default.weight\n",
      "roberta.encoder.layer.9.attention.self.key.lora_B.default.weight\n",
      "roberta.encoder.layer.9.attention.self.value.base_layer.weight\n",
      "roberta.encoder.layer.9.attention.self.value.base_layer.bias\n",
      "roberta.encoder.layer.9.attention.self.value.lora_A.default.weight\n",
      "roberta.encoder.layer.9.attention.self.value.lora_B.default.weight\n",
      "roberta.encoder.layer.9.attention.output.dense.weight\n",
      "roberta.encoder.layer.9.attention.output.dense.bias\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.intermediate.dense.weight\n",
      "roberta.encoder.layer.9.intermediate.dense.bias\n",
      "roberta.encoder.layer.9.output.dense.weight\n",
      "roberta.encoder.layer.9.output.dense.bias\n",
      "roberta.encoder.layer.9.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.attention.self.query.base_layer.weight\n",
      "roberta.encoder.layer.10.attention.self.query.base_layer.bias\n",
      "roberta.encoder.layer.10.attention.self.query.lora_A.default.weight\n",
      "roberta.encoder.layer.10.attention.self.query.lora_B.default.weight\n",
      "roberta.encoder.layer.10.attention.self.key.base_layer.weight\n",
      "roberta.encoder.layer.10.attention.self.key.base_layer.bias\n",
      "roberta.encoder.layer.10.attention.self.key.lora_A.default.weight\n",
      "roberta.encoder.layer.10.attention.self.key.lora_B.default.weight\n",
      "roberta.encoder.layer.10.attention.self.value.base_layer.weight\n",
      "roberta.encoder.layer.10.attention.self.value.base_layer.bias\n",
      "roberta.encoder.layer.10.attention.self.value.lora_A.default.weight\n",
      "roberta.encoder.layer.10.attention.self.value.lora_B.default.weight\n",
      "roberta.encoder.layer.10.attention.output.dense.weight\n",
      "roberta.encoder.layer.10.attention.output.dense.bias\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.intermediate.dense.weight\n",
      "roberta.encoder.layer.10.intermediate.dense.bias\n",
      "roberta.encoder.layer.10.output.dense.weight\n",
      "roberta.encoder.layer.10.output.dense.bias\n",
      "roberta.encoder.layer.10.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.attention.self.query.base_layer.weight\n",
      "roberta.encoder.layer.11.attention.self.query.base_layer.bias\n",
      "roberta.encoder.layer.11.attention.self.query.lora_A.default.weight\n",
      "roberta.encoder.layer.11.attention.self.query.lora_B.default.weight\n",
      "roberta.encoder.layer.11.attention.self.key.base_layer.weight\n",
      "roberta.encoder.layer.11.attention.self.key.base_layer.bias\n",
      "roberta.encoder.layer.11.attention.self.key.lora_A.default.weight\n",
      "roberta.encoder.layer.11.attention.self.key.lora_B.default.weight\n",
      "roberta.encoder.layer.11.attention.self.value.base_layer.weight\n",
      "roberta.encoder.layer.11.attention.self.value.base_layer.bias\n",
      "roberta.encoder.layer.11.attention.self.value.lora_A.default.weight\n",
      "roberta.encoder.layer.11.attention.self.value.lora_B.default.weight\n",
      "roberta.encoder.layer.11.attention.output.dense.weight\n",
      "roberta.encoder.layer.11.attention.output.dense.bias\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.intermediate.dense.weight\n",
      "roberta.encoder.layer.11.intermediate.dense.bias\n",
      "roberta.encoder.layer.11.output.dense.weight\n",
      "roberta.encoder.layer.11.output.dense.bias\n",
      "roberta.encoder.layer.11.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.output.LayerNorm.bias\n",
      "classifier.original_module.dense.weight\n",
      "classifier.original_module.dense.bias\n",
      "classifier.original_module.out_proj.weight\n",
      "classifier.original_module.out_proj.bias\n",
      "classifier.modules_to_save.default.dense.weight\n",
      "classifier.modules_to_save.default.dense.bias\n",
      "classifier.modules_to_save.default.out_proj.weight\n",
      "classifier.modules_to_save.default.out_proj.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "from transformers import Trainer\n",
    "\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        probs = probs.gather(dim=-1, index=targets.unsqueeze(-1))\n",
    "        probs = probs.squeeze(-1)\n",
    "        loss = -((1 - probs) ** self.gamma) * torch.log(probs)\n",
    "        return loss.mean()\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fn = FocalLoss(gamma=2.0)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='5136' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  27/5136 00:16 < 57:07, 1.49 it/s, Epoch 0.04/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 38\u001b[0m\n\u001b[0;32m     21\u001b[0m lr_scheduler \u001b[38;5;241m=\u001b[39m get_scheduler(\n\u001b[0;32m     22\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     23\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mAdamW(peft_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mtraining_args\u001b[38;5;241m.\u001b[39mlearning_rate),\n\u001b[0;32m     24\u001b[0m     num_warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m num_training_steps),\n\u001b[0;32m     25\u001b[0m     num_training_steps\u001b[38;5;241m=\u001b[39mnum_training_steps,\n\u001b[0;32m     26\u001b[0m )\n\u001b[0;32m     28\u001b[0m trainer \u001b[38;5;241m=\u001b[39m CustomTrainer(\n\u001b[0;32m     29\u001b[0m     model\u001b[38;5;241m=\u001b[39mpeft_model,\n\u001b[0;32m     30\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m     optimizers\u001b[38;5;241m=\u001b[39m(AdamW(peft_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mtraining_args\u001b[38;5;241m.\u001b[39mlearning_rate), lr_scheduler),\n\u001b[0;32m     34\u001b[0m )\n\u001b[1;32m---> 38\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\transformers\\trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\transformers\\trainer.py:2553\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2547\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m   2548\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m   2550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2551\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m-> 2553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2554\u001b[0m ):\n\u001b[0;32m   2555\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2556\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2557\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=8,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    push_to_hub=False,\n",
    "    label_names=[\"labels\"],\n",
    "    optim=\"adamw_torch\",  \n",
    "    lr_scheduler_type=\"cosine\"  \n",
    ")\n",
    "\n",
    "num_training_steps = len(train_dataloader) * training_args.num_train_epochs\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"cosine\",\n",
    "    optimizer=AdamW(peft_model.parameters(), lr=training_args.learning_rate),\n",
    "    num_warmup_steps=int(0.1 * num_training_steps),\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    optimizers=(AdamW(peft_model.parameters(), lr=training_args.learning_rate), lr_scheduler),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='161' max='161' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [161/161 00:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Model Accuracy: 0.2858\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics  \n",
    ")\n",
    "\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "accuracy = eval_results[\"eval_accuracy\"]\n",
    "print(f\"Trained Model Accuracy: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
